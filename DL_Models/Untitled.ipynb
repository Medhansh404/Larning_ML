{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "732a13e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 143\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parameters\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Train the neural network\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m trained_parameters \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_neural_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# Test the neural network on the test set\u001b[39;00m\n\u001b[0;32m    146\u001b[0m test_predictions, _ \u001b[38;5;241m=\u001b[39m forward_propagation(X_test, trained_parameters)\n",
      "Cell \u001b[1;32mIn[8], line 133\u001b[0m, in \u001b[0;36mtrain_neural_network\u001b[1;34m(X, modf_Y, layer_dims, learning_rate, num_iterations)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[0;32m    132\u001b[0m     AL, caches \u001b[38;5;241m=\u001b[39m forward_propagation(X, parameters)\n\u001b[1;32m--> 133\u001b[0m     cost \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_cost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m     grads \u001b[38;5;241m=\u001b[39m backward_propagation(AL, Y, caches)\n\u001b[0;32m    135\u001b[0m     parameters \u001b[38;5;241m=\u001b[39m update_parameters(parameters, grads, learning_rate)\n",
      "Cell \u001b[1;32mIn[8], line 66\u001b[0m, in \u001b[0;36mcompute_cost\u001b[1;34m(AL, Y)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_cost\u001b[39m(AL, Y):\n\u001b[1;32m---> 66\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[43mY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     67\u001b[0m     cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m m) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(Y \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(AL) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m Y) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m AL))\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cost\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from functools import reduce \n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "    parameters = {}\n",
    "    for l in range(1, len(layer_dims)):\n",
    "        parameters[f\"W{l}\"] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        parameters[f\"b{l}\"] = np.zeros((layer_dims[l], 1))\n",
    "    return parameters\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def modified_Y(Y):\n",
    "    modf_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    modf_Y[np.arange(Y.size), Y] = 1\n",
    "    modf_Y = modf_Y.T\n",
    "    return modf_Y\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    for l in range(1, len(parameters) // 2):\n",
    "        W, b = parameters[f\"W{l}\"], parameters[f\"b{l}\"]\n",
    "        Z = np.dot(W, A) + b\n",
    "        A = sigmoid(Z)\n",
    "        caches.append((A, W, b, Z))\n",
    "    \n",
    "    WL, bL = parameters[f\"W{len(parameters) // 2}\"], parameters[f\"b{len(parameters) // 2}\"]\n",
    "    AL = sigmoid(np.dot(WL, A) + bL)\n",
    "    caches.append((AL, WL, bL, np.dot(WL, A) + bL))\n",
    "    return AL, caches\n",
    "\n",
    "def backward_propagation(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "    A_prev, W, b, Z = caches[L - 1]\n",
    "    dZ = dAL * sigmoid_prime(Z)\n",
    "    grads[f\"dW{L}\"] = np.dot(dZ, A_prev.T) / m\n",
    "    grads[f\"db{L}\"] = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    grads[f\"dA{L - 1}\"] = np.dot(W.T, dZ)\n",
    "\n",
    "    for l in reversed(range(1, L - 1)):\n",
    "        A_prev, W, b, Z = caches[l]\n",
    "        dZ = grads[f\"dA{L - 1}\"] * sigmoid_prime(Z)  # Fix indexing here\n",
    "        grads[f\"dW{l}\"] = np.dot(dZ, A_prev.T) / m\n",
    "        grads[f\"db{l}\"] = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "        grads[f\"dA{l - 1}\"] = np.dot(W.T, dZ)\n",
    "\n",
    "    # Handle the first layer separately\n",
    "    A0, W0, b0, Z0 = caches[0]\n",
    "    dZ0 = grads[\"dA0\"] * sigmoid_prime(Z0)\n",
    "    grads[\"dW0\"] = np.dot(dZ0, A0.T) / m\n",
    "    grads[\"db0\"] = np.sum(dZ0, axis=1, keepdims=True) / m\n",
    "\n",
    "    return grads\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = - (1 / m) * np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))\n",
    "    return cost\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    for l in range(1, len(parameters) // 2 + 1):\n",
    "        parameters[f\"W{l}\"] -= learning_rate * grads[f\"dW{l}\"]\n",
    "        parameters[f\"b{l}\"] -= learning_rate * grads[f\"db{l}\"]\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def train_iteration(i, parameters, X, Y, learning_rate):\n",
    "    AL, caches = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(AL, Y)\n",
    "    grads = backward_propagation(AL, Y, caches)\n",
    "    parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f'Cost after iteration {i}: {cost}')\n",
    "\n",
    "    return parameters\n",
    "\n",
    "modf_Y = modified_Y(Y)\n",
    "\n",
    "def train_neural_network(X, Y, layer_dims, learning_rate, num_iterations):\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    trained_parameters = reduce(lambda params, i: train_iteration(i, params, X, Y, learning_rate),\n",
    "                                range(num_iterations),\n",
    "                                parameters)\n",
    "    return trained_parameters\n",
    "\n",
    "# Load and preprocess data (simplified for demonstration)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv(\"./MNIST.csv\")\n",
    "data = np.array(data)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "m, n = data.shape\n",
    "\n",
    "Y = data[:, 0]\n",
    "X = data[:, 1:]\n",
    "X = X / 255.0\n",
    "\n",
    "num_test_samples = 1000\n",
    "X_test = X[:num_test_samples, :].T\n",
    "Y_test = Y[:num_test_samples]\n",
    "\n",
    "X_train = X[num_test_samples:, :].T\n",
    "Y_train = Y[num_test_samples:]\n",
    "Y_train = Y_train.reshape(1, -1)\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "\n",
    "# Define neural network architecture\n",
    "layer_dims = [X_train.shape[0], 20, 10, 1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_neural_network(X, modf_Y, layer_dims, learning_rate, num_iterations):\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    for i in range(num_iterations):\n",
    "        AL, caches = forward_propagation(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = backward_propagation(AL, Y, caches)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'Cost after iteration {i}: {cost}')\n",
    "\n",
    "    return parameters\n",
    "\n",
    "# Train the neural network\n",
    "trained_parameters = train_neural_network(X_train, Y_train, layer_dims, learning_rate=0.01, num_iterations=1000)\n",
    "\n",
    "# Test the neural network on the test set\n",
    "test_predictions, _ = forward_propagation(X_test, trained_parameters)\n",
    "test_predictions = (test_predictions > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = np.mean(test_predictions == Y_test)\n",
    "print(f\"Accuracy on test set: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdb533a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
